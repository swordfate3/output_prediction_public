#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mamba æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ CipherMamba æ¨¡å‹è¿›è¡Œå¯†æ–‡é¢„æµ‹ä»»åŠ¡

Author: Output Prediction Project
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
from models.mamba_model import CipherMamba, MambaBlock, SelectiveScan


def test_selective_scan():
    """
    æµ‹è¯•é€‰æ‹©æ€§æ‰«ææœºåˆ¶
    
    éªŒè¯ SelectiveScan ç»„ä»¶çš„åŸºæœ¬åŠŸèƒ½
    """
    print("=== æµ‹è¯•é€‰æ‹©æ€§æ‰«ææœºåˆ¶ ===")
    
    # åˆ›å»ºé€‰æ‹©æ€§æ‰«æå±‚
    d_model = 64
    scan_layer = SelectiveScan(d_model=d_model, d_state=16, d_conv=4, expand=2)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 4, 8
    x = torch.randn(batch_size, seq_len, d_model)
    
    # å‰å‘ä¼ æ’­
    output = scan_layer(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"é€‰æ‹©æ€§æ‰«ææµ‹è¯•é€šè¿‡ âœ“")
    print()


def test_mamba_block():
    """
    æµ‹è¯• Mamba åŸºç¡€å—
    
    éªŒè¯ MambaBlock çš„åŠŸèƒ½å’Œæ®‹å·®è¿æ¥
    """
    print("=== æµ‹è¯• Mamba åŸºç¡€å— ===")
    
    # åˆ›å»º Mamba å—
    d_model = 128
    mamba_block = MambaBlock(d_model=d_model, d_state=16, d_conv=4, expand=2)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 2, 10
    x = torch.randn(batch_size, seq_len, d_model)
    
    # å‰å‘ä¼ æ’­
    output = mamba_block(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ®‹å·®è¿æ¥éªŒè¯: {output.shape == x.shape}")
    print(f"Mamba å—æµ‹è¯•é€šè¿‡ âœ“")
    print()


def test_cipher_mamba():
    """
    æµ‹è¯•å®Œæ•´çš„ CipherMamba æ¨¡å‹
    
    éªŒè¯æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†åŠŸèƒ½
    """
    print("=== æµ‹è¯• CipherMamba æ¨¡å‹ ===")
    
    # æ¨¡å‹å‚æ•°
    input_dim = 16  # 16ä½è¾“å…¥
    hidden_dim = 256
    num_layers = 4
    output_dim = 16  # 16ä½è¾“å‡º
    
    # åˆ›å»ºæ¨¡å‹
    model = CipherMamba(
        input_dim=input_dim,
        hidden_dim=hidden_dim,
        num_layers=num_layers,
        output_dim=output_dim,
        d_state=16,
        d_conv=4,
        expand=2,
        dropout=0.1
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 8, 1  # å¯†ç å­¦ä»»åŠ¡é€šå¸¸åºåˆ—é•¿åº¦ä¸º1
    x = torch.randn(batch_size, seq_len, input_dim)
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        output = model(x)
    
    print(f"æ¨¡å‹å‚æ•°:")
    info = model.getModelInfo()
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
    print(f"CipherMamba æ¨¡å‹æµ‹è¯•é€šè¿‡ âœ“")
    print()


def test_model_save_load():
    """
    æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½
    
    éªŒè¯æ¨¡å‹çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–
    """
    print("=== æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ ===")
    
    # åˆ›å»ºåŸå§‹æ¨¡å‹
    original_model = CipherMamba(
        input_dim=16,
        hidden_dim=128,
        num_layers=2,
        output_dim=16
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_input = torch.randn(1, 1, 16)
    
    # è·å–åŸå§‹è¾“å‡º
    original_model.eval()
    with torch.no_grad():
        original_output = original_model(test_input)
    
    # ä¿å­˜æ¨¡å‹
    save_path = "/tmp/test_mamba_model.pth"
    original_model.saveModel(save_path, epoch=10, loss=0.1)
    
    # åŠ è½½æ¨¡å‹
    loaded_model, checkpoint = CipherMamba.loadModel(save_path)
    
    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    loaded_model.eval()
    with torch.no_grad():
        loaded_output = loaded_model(test_input)
    
    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´
    output_match = torch.allclose(original_output, loaded_output, atol=1e-6)
    
    print(f"æ£€æŸ¥ç‚¹ä¿¡æ¯:")
    print(f"  è½®æ¬¡: {checkpoint['epoch']}")
    print(f"  æŸå¤±: {checkpoint['loss']}")
    print(f"è¾“å‡ºä¸€è‡´æ€§: {output_match}")
    print(f"æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•é€šè¿‡ âœ“")
    print()


def performance_comparison():
    """
    æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    
    æ¯”è¾ƒ Mamba å’Œ LSTM æ¨¡å‹çš„å‚æ•°é‡å’Œæ¨ç†é€Ÿåº¦
    """
    print("=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    # åˆ›å»ºç›¸ä¼¼è§„æ¨¡çš„æ¨¡å‹
    input_dim, output_dim = 16, 16
    hidden_dim = 256
    
    # Mamba æ¨¡å‹
    mamba_model = CipherMamba(
        input_dim=input_dim,
        hidden_dim=hidden_dim,
        num_layers=4,
        output_dim=output_dim
    )
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    mamba_info = mamba_model.getModelInfo()
    
    print(f"Mamba æ¨¡å‹:")
    print(f"  å‚æ•°æ€»æ•°: {mamba_info['total_params']:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {mamba_info['trainable_params']:,}")
    
    # æ¨ç†é€Ÿåº¦æµ‹è¯•
    test_input = torch.randn(32, 1, input_dim)
    
    # é¢„çƒ­
    mamba_model.eval()
    with torch.no_grad():
        _ = mamba_model(test_input)
    
    # è®¡æ—¶
    import time
    start_time = time.time()
    with torch.no_grad():
        for _ in range(100):
            _ = mamba_model(test_input)
    mamba_time = time.time() - start_time
    
    print(f"  æ¨ç†æ—¶é—´ (100æ¬¡): {mamba_time:.4f}s")
    print(f"  å¹³å‡æ¨ç†æ—¶é—´: {mamba_time/100*1000:.2f}ms")
    print()


def main():
    """
    ä¸»å‡½æ•°
    
    è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
    """
    print("Mamba æ¨¡å‹æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    try:
        # è¿è¡Œå„é¡¹æµ‹è¯•
        test_selective_scan()
        test_mamba_block()
        test_cipher_mamba()
        test_model_save_load()
        performance_comparison()
        
        print("=" * 50)
        print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Mamba æ¨¡å‹å®ç°æˆåŠŸ ğŸ‰")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()