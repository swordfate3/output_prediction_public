#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„ Mamba æ¨¡å‹æµ‹è¯•

éªŒè¯ Mamba æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½ï¼Œä¸ä¾èµ– einops

Author: Output Prediction Project
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn


class SimpleMambaTest(nn.Module):
    """
    ç®€åŒ–çš„ Mamba æµ‹è¯•æ¨¡å‹
    
    ç”¨äºéªŒè¯åŸºæœ¬çš„æ¨¡å‹ç»“æ„å’Œæ¥å£
    """
    
    def __init__(self, input_dim=16, hidden_dim=256, output_dim=16):
        """
        åˆå§‹åŒ–ç®€åŒ–æµ‹è¯•æ¨¡å‹
        
        Args:
            input_dim (int): è¾“å…¥ç»´åº¦
            hidden_dim (int): éšè—å±‚ç»´åº¦
            output_dim (int): è¾“å‡ºç»´åº¦
        """
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # ç®€åŒ–çš„çº¿æ€§å±‚æ›¿ä»£å¤æ‚çš„ Mamba ç»“æ„
        self.input_proj = nn.Linear(input_dim, hidden_dim)
        self.hidden_layers = nn.ModuleList([
            nn.Linear(hidden_dim, hidden_dim) for _ in range(4)
        ])
        self.output_proj = nn.Linear(hidden_dim, output_dim)
        self.activation = nn.SiLU()
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ [batch_size, seq_len, input_dim]
            
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ [batch_size, seq_len, output_dim]
        """
        # è¾“å…¥æŠ•å½±
        x = self.input_proj(x)
        x = self.activation(x)
        
        # éšè—å±‚å¤„ç†
        for layer in self.hidden_layers:
            residual = x
            x = layer(x)
            x = self.activation(x)
            x = self.dropout(x)
            x = x + residual  # æ®‹å·®è¿æ¥
        
        # è¾“å‡ºæŠ•å½±
        x = self.output_proj(x)
        return torch.sigmoid(x)  # å¯†ç å­¦ä»»åŠ¡é€šå¸¸éœ€è¦ 0-1 è¾“å‡º
    
    def saveModel(self, path, epoch=0, loss=0.0):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            path (str): ä¿å­˜è·¯å¾„
            epoch (int): è®­ç»ƒè½®æ¬¡
            loss (float): æŸå¤±å€¼
        """
        checkpoint = {
            'model_state_dict': self.state_dict(),
            'model_config': {
                'input_dim': self.input_dim,
                'hidden_dim': self.hidden_dim,
                'output_dim': self.output_dim
            },
            'epoch': epoch,
            'loss': loss
        }
        torch.save(checkpoint, path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")
    
    @classmethod
    def loadModel(cls, path):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            path (str): æ¨¡å‹è·¯å¾„
            
        Returns:
            tuple: (æ¨¡å‹å®ä¾‹, æ£€æŸ¥ç‚¹ä¿¡æ¯)
        """
        checkpoint = torch.load(path, map_location='cpu')
        config = checkpoint['model_config']
        
        model = cls(
            input_dim=config['input_dim'],
            hidden_dim=config['hidden_dim'],
            output_dim=config['output_dim']
        )
        
        model.load_state_dict(checkpoint['model_state_dict'])
        return model, checkpoint
    
    def getModelInfo(self):
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            dict: æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'SimpleMambaTest',
            'input_dim': self.input_dim,
            'hidden_dim': self.hidden_dim,
            'output_dim': self.output_dim,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': total_params * 4 / (1024 * 1024)  # å‡è®¾ float32
        }


def test_model_basic():
    """
    æµ‹è¯•æ¨¡å‹åŸºæœ¬åŠŸèƒ½
    """
    print("=== æµ‹è¯•æ¨¡å‹åŸºæœ¬åŠŸèƒ½ ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleMambaTest(input_dim=16, hidden_dim=256, output_dim=16)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 8, 1
    x = torch.randn(batch_size, seq_len, 16)
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        output = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
    
    # æ¨¡å‹ä¿¡æ¯
    info = model.getModelInfo()
    print(f"æ¨¡å‹ä¿¡æ¯:")
    for key, value in info.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value:,}" if isinstance(value, int) else f"  {key}: {value}")
    
    print("åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ âœ“\n")


def test_model_save_load():
    """
    æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
    """
    print("=== æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½ ===")
    
    # åˆ›å»ºåŸå§‹æ¨¡å‹
    original_model = SimpleMambaTest(input_dim=16, hidden_dim=128, output_dim=16)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_input = torch.randn(1, 1, 16)
    
    # è·å–åŸå§‹è¾“å‡º
    original_model.eval()
    with torch.no_grad():
        original_output = original_model(test_input)
    
    # ä¿å­˜æ¨¡å‹
    save_path = "/tmp/test_simple_mamba.pth"
    original_model.saveModel(save_path, epoch=10, loss=0.1)
    
    # åŠ è½½æ¨¡å‹
    loaded_model, checkpoint = SimpleMambaTest.loadModel(save_path)
    
    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    loaded_model.eval()
    with torch.no_grad():
        loaded_output = loaded_model(test_input)
    
    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´
    output_match = torch.allclose(original_output, loaded_output, atol=1e-6)
    
    print(f"æ£€æŸ¥ç‚¹ä¿¡æ¯:")
    print(f"  è½®æ¬¡: {checkpoint['epoch']}")
    print(f"  æŸå¤±: {checkpoint['loss']}")
    print(f"è¾“å‡ºä¸€è‡´æ€§: {output_match}")
    print("ä¿å­˜å’ŒåŠ è½½æµ‹è¯•é€šè¿‡ âœ“\n")


def test_training_simulation():
    """
    æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    """
    print("=== æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ ===")
    
    # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = SimpleMambaTest(input_dim=16, hidden_dim=128, output_dim=16)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size = 32
    x = torch.randn(batch_size, 1, 16)
    y = torch.randn(batch_size, 1, 16)
    
    # è®­ç»ƒå‡ æ­¥
    model.train()
    initial_loss = None
    final_loss = None
    
    for epoch in range(5):
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        
        if epoch == 0:
            initial_loss = loss.item()
        if epoch == 4:
            final_loss = loss.item()
        
        print(f"Epoch {epoch+1}/5, Loss: {loss.item():.6f}")
    
    print(f"åˆå§‹æŸå¤±: {initial_loss:.6f}")
    print(f"æœ€ç»ˆæŸå¤±: {final_loss:.6f}")
    print(f"æŸå¤±ä¸‹é™: {initial_loss - final_loss:.6f}")
    print("è®­ç»ƒæ¨¡æ‹Ÿæµ‹è¯•é€šè¿‡ âœ“\n")


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ç®€åŒ– Mamba æ¨¡å‹æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    try:
        test_model_basic()
        test_model_save_load()
        test_training_simulation()
        
        print("=" * 50)
        print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹æ¥å£éªŒè¯æˆåŠŸ ğŸ‰")
        print("\næ³¨æ„: è¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬çš„æµ‹è¯•ï¼Œç”¨äºéªŒè¯æ¨¡å‹æ¥å£ã€‚")
        print("å®Œæ•´çš„ Mamba æ¨¡å‹å®ç°åœ¨ models/mamba_model.py ä¸­ã€‚")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()